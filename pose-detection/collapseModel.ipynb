{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1bc92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey/Documents/Coding/github/berkeley-ai-hackathon-2025/.venv/lib/python3.12/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load MoveNet\n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "movenet = model.signatures['serving_default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ece1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pose(frame):\n",
    "    input_image = tf.image.resize_with_pad(tf.expand_dims(frame, axis=0), 256, 256)\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "\n",
    "    # Run model\n",
    "    results = movenet(input_image)\n",
    "    keypoints = results['output_0'].numpy()[0, 0, :, :2]  # shape: (17, 2)\n",
    "\n",
    "    return keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d90a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey/Documents/Coding/github/berkeley-ai-hackathon-2025/.venv/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, InputLayer\n",
    "\n",
    "# MLP Model\n",
    "mlp_model = Sequential([\n",
    "    InputLayer(input_shape=(34,)),  # 17 keypoints * (x, y)\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Output: collapse score between 0 and 1\n",
    "])\n",
    "mlp_model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9206b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_keypoints(keypoints):\n",
    "    # Flatten and normalize coordinates to [0, 1]\n",
    "    flattened = keypoints.flatten()\n",
    "    return flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e7a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Or use a file path\n",
    "time.sleep(2)  # Allow camera to warm up\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame for MoveNet\n",
    "    input_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    keypoints = detect_pose(input_frame)\n",
    "    input_vec = preprocess_keypoints(keypoints)\n",
    "\n",
    "    # Predict collapse score\n",
    "    collapse_score = mlp_model.predict(np.expand_dims(input_vec, axis=0))[0][0]\n",
    "\n",
    "    # Display\n",
    "    cv2.putText(frame, f'Collapse Score: {collapse_score:.2f}', (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow('Pose + Collapse Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a535fd",
   "metadata": {},
   "source": [
    "# Keypoint Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38ec721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keypoint connections (from COCO format)\n",
    "SKELETON = [\n",
    "    (0, 1), (1, 3), (0, 2), (2, 4),       # Head -> Shoulders -> Arms\n",
    "    (5, 7), (7, 9), (6, 8), (8, 10),      # Arms\n",
    "    (5, 6), (5, 11), (6, 12),             # Torso\n",
    "    (11, 12), (11, 13), (13, 15),         # Legs\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_pose(frame, keypoints):\n",
    "    h, w, _ = frame.shape\n",
    "    # Convert normalized coordinates to pixel\n",
    "    keypoints_px = [(int(x * w), int(y * h)) for y, x in keypoints]\n",
    "\n",
    "    # Draw bones\n",
    "    for start, end in SKELETON:\n",
    "        x1, y1 = keypoints_px[start]\n",
    "        x2, y2 = keypoints_px[end]\n",
    "        cv2.line(frame, (x1, y1), (x2, y2), color=(0, 255, 255), thickness=2)\n",
    "\n",
    "    # Draw joints\n",
    "    for (x, y) in keypoints_px:\n",
    "        cv2.circle(frame, (x, y), radius=5, color=(0, 0, 255), thickness=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e508a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load MoveNet Thunder model\n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "movenet = model.signatures['serving_default']\n",
    "\n",
    "# Output file setup\n",
    "output_file = \"pose_labelsTieShoe.csv\"\n",
    "if not os.path.exists(output_file):\n",
    "    df = pd.DataFrame(columns=[f'kpt_{i}' for i in range(34)] + ['label'])  # 17 keypoints * (x, y) + label\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "def detect_pose(frame):\n",
    "    input_image = tf.image.resize_with_pad(tf.expand_dims(frame, axis=0), 256, 256)\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    outputs = movenet(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()[0, 0, :, :2]  # shape: (17, 2)\n",
    "    return keypoints\n",
    "\n",
    "def preprocess_keypoints(keypoints):\n",
    "    return keypoints.flatten()  # shape: (34,)\n",
    "\n",
    "def append_to_csv(keypoints, label, file_path):\n",
    "    flattened = preprocess_keypoints(keypoints)\n",
    "    row = np.append(flattened, label)\n",
    "    df = pd.DataFrame([row])\n",
    "    df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa83b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'z' to save as GOOD pose, 'x' as COLLAPSED pose, 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    keypoints = detect_pose(rgb_frame)\n",
    "\n",
    "    # Draw keypoints on frame\n",
    "    # for x, y in keypoints:\n",
    "    #     h, w, _ = frame.shape\n",
    "    #     cx, cy = int(x * w), int(y * h)\n",
    "    #     cv2.circle(frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "    draw_pose(frame, keypoints)\n",
    "\n",
    "    cv2.imshow(\"MoveNet Pose Capture\", frame)\n",
    "\n",
    "    key = cv2.waitKey(10) & 0xFF\n",
    "    if key == ord('z'):\n",
    "        print(\"Saved GOOD pose.\")\n",
    "        append_to_csv(keypoints, 0, output_file)\n",
    "    elif key == ord('x'):\n",
    "        print(\"Saved COLLAPSED pose.\")\n",
    "        append_to_csv(keypoints, 1, output_file)\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf4bb4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6532a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb36bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the CSV\n",
    "df0 = pd.read_csv(\"pose_labels.csv\")\n",
    "\n",
    "df1 = pd.read_csv(\"pose_labelsTieShoe.csv\")\n",
    "\n",
    "\n",
    "# df = pd.concat([df0, df1], axis=0)\n",
    "\n",
    "df = df0\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(\"label\", axis=1).values  # shape: (N, 34)\n",
    "y = df[\"label\"].values               # shape: (N,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b26f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalize features (0 mean, 1 std)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07010efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 0.6250 - loss: 0.6978 - val_accuracy: 0.6250 - val_loss: 0.7376\n",
      "Epoch 2/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5833 - loss: 0.6223 - val_accuracy: 0.6250 - val_loss: 0.6773\n",
      "Epoch 3/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6042 - loss: 0.6608 - val_accuracy: 0.6250 - val_loss: 0.6188\n",
      "Epoch 4/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5833 - loss: 0.5978 - val_accuracy: 0.6250 - val_loss: 0.5632\n",
      "Epoch 5/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7292 - loss: 0.5426 - val_accuracy: 0.8750 - val_loss: 0.5127\n",
      "Epoch 6/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7292 - loss: 0.5151 - val_accuracy: 1.0000 - val_loss: 0.4657\n",
      "Epoch 7/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9375 - loss: 0.3934 - val_accuracy: 1.0000 - val_loss: 0.4254\n",
      "Epoch 8/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9375 - loss: 0.3758 - val_accuracy: 1.0000 - val_loss: 0.3904\n",
      "Epoch 9/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7708 - loss: 0.4238 - val_accuracy: 1.0000 - val_loss: 0.3585\n",
      "Epoch 10/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8750 - loss: 0.3841 - val_accuracy: 1.0000 - val_loss: 0.3307\n",
      "Epoch 11/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9583 - loss: 0.3118 - val_accuracy: 1.0000 - val_loss: 0.3070\n",
      "Epoch 12/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9583 - loss: 0.2757 - val_accuracy: 1.0000 - val_loss: 0.2845\n",
      "Epoch 13/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9583 - loss: 0.2746 - val_accuracy: 1.0000 - val_loss: 0.2643\n",
      "Epoch 14/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9375 - loss: 0.3216 - val_accuracy: 1.0000 - val_loss: 0.2455\n",
      "Epoch 15/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9792 - loss: 0.2296 - val_accuracy: 1.0000 - val_loss: 0.2284\n",
      "Epoch 16/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.2402 - val_accuracy: 1.0000 - val_loss: 0.2122\n",
      "Epoch 17/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9583 - loss: 0.2304 - val_accuracy: 1.0000 - val_loss: 0.1977\n",
      "Epoch 18/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.1782 - val_accuracy: 1.0000 - val_loss: 0.1848\n",
      "Epoch 19/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.1712 - val_accuracy: 1.0000 - val_loss: 0.1726\n",
      "Epoch 20/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.1635 - val_accuracy: 1.0000 - val_loss: 0.1610\n",
      "Epoch 21/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.1813 - val_accuracy: 1.0000 - val_loss: 0.1495\n",
      "Epoch 22/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9792 - loss: 0.1500 - val_accuracy: 1.0000 - val_loss: 0.1385\n",
      "Epoch 23/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9583 - loss: 0.1416 - val_accuracy: 1.0000 - val_loss: 0.1281\n",
      "Epoch 24/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9583 - loss: 0.1492 - val_accuracy: 1.0000 - val_loss: 0.1185\n",
      "Epoch 25/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.1226 - val_accuracy: 1.0000 - val_loss: 0.1096\n",
      "Epoch 26/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9792 - loss: 0.1711 - val_accuracy: 1.0000 - val_loss: 0.1014\n",
      "Epoch 27/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.1327 - val_accuracy: 1.0000 - val_loss: 0.0938\n",
      "Epoch 28/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0833 - val_accuracy: 1.0000 - val_loss: 0.0870\n",
      "Epoch 29/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.1005 - val_accuracy: 1.0000 - val_loss: 0.0805\n",
      "Epoch 30/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0882 - val_accuracy: 1.0000 - val_loss: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "# Define MLP model\n",
    "model = Sequential([\n",
    "    Input(shape=(34,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Output: collapse score ∈ [0, 1]\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save('collapseModel.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0d730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0748\n",
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b53a64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keypoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# After detecting keypoints:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m input_vec = preprocess_keypoints(\u001b[43mkeypoints\u001b[49m).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m      3\u001b[39m input_vec = scaler.transform(input_vec)  \u001b[38;5;66;03m# Use the same scaler\u001b[39;00m\n\u001b[32m      5\u001b[39m collapse_score = model.predict(input_vec)[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# Output: 0-1\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'keypoints' is not defined"
     ]
    }
   ],
   "source": [
    "# After detecting keypoints:\n",
    "input_vec = preprocess_keypoints(keypoints).reshape(1, -1)\n",
    "input_vec = scaler.transform(input_vec)  # Use the same scaler\n",
    "\n",
    "collapse_score = model.predict(input_vec)[0][0]  # Output: 0-1\n",
    "\n",
    "print(f\"Predicted collapse score: {collapse_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4293e00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: not authorized to capture video (status 0), requesting...\n",
      "OpenCV: camera failed to properly initialize!\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR to RGB for MoveNet\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    keypoints = detect_pose(rgb_frame)\n",
    "    input_vec = preprocess_keypoints(keypoints).reshape(1, -1)\n",
    "    input_vec = scaler.transform(input_vec)  # Use the same scaler as training\n",
    "\n",
    "    # Predict collapse score\n",
    "    collapse_score = model.predict(input_vec)[0][0]\n",
    "\n",
    "    # Draw pose\n",
    "    draw_pose(frame, keypoints)\n",
    "\n",
    "    # Display collapse score\n",
    "    cv2.putText(frame, f'Collapse Score: {collapse_score:.2f}', (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow('Live Pose + Collapse Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e8e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
